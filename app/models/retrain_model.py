# app/models/retrain_model.py
import hashlib
import json
from pathlib import Path
import pandas as pd
from app.models.train_model import train, simple_clean_text
from typing import Union

def compute_hash(text: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ö—ç—à –¥–ª—è —Å—Ç—Ä–æ–∫–∏ (—Ç–µ–∫—Å—Ç–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è)."""
    return hashlib.sha256(text.strip().lower().encode("utf-8")).hexdigest()


def append_new_data(
    new_csv_path: str,
    master_csv_path: str = "data/training_data.csv",
    used_hashes_path: str = "data/used_hashes.txt",
):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ confident –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤ –æ–±—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∞–ª—å–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫.
    """
    new_csv = Path(new_csv_path)
    master_csv = Path(master_csv_path)
    used_hashes = Path(used_hashes_path)

    # —Å–æ–∑–¥–∞—ë–º, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    master_csv.parent.mkdir(parents=True, exist_ok=True)
    if not master_csv.exists():
        master_df = pd.DataFrame(columns=["description", "category_id"])
    else:
        master_df = pd.read_csv(master_csv)

    # —á–∏—Ç–∞–µ–º –Ω–æ–≤—ã–µ confident –¥–∞–Ω–Ω—ã–µ
    new_df = pd.read_csv(new_csv)
    if "description" not in new_df.columns or "category_id" not in new_df.columns:
        raise ValueError("CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'description' –∏ 'category_id'")

    # —á–∏—Ç–∞–µ–º —Ö—ç—à–∏ —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    used = set()
    if used_hashes.exists():
        with used_hashes.open("r", encoding="utf-8") as f:
            used = {line.strip() for line in f if line.strip()}

    # –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ
    added_rows = []
    for _, row in new_df.iterrows():
        desc = str(row["description"])
        h = compute_hash(desc)
        if h not in used:
            added_rows.append(row)
            used.add(h)

    if not added_rows:
        print("üîπ –ù–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ—Ç.")
        return 0

    added_df = pd.DataFrame(added_rows)
    combined = pd.concat([master_df, added_df], ignore_index=True)

    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏ —Ö—ç—à–∏
    combined.to_csv(master_csv, index=False)
    with used_hashes.open("w", encoding="utf-8") as f:
        f.write("\n".join(sorted(used)))

    print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(added_rows)} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π (–≤—Å–µ–≥–æ —Ç–µ–ø–µ—Ä—å {len(combined)})")
    return len(added_rows)


def retrain_with_new_data(
    new_csv_path: str,
    categories_path: Union[str, None] = None,
    out_dir: str = "data/models",
):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏:
    1. –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ confident –¥–∞–Ω–Ω—ã–µ –≤ –æ–±—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
    2. –≤—ã–∑—ã–≤–∞–µ—Ç train() –∏–∑ train_model.py
    """
    added = append_new_data(new_csv_path)
    if added == 0:
        print("‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –Ω–µ –∑–∞–ø—É—â–µ–Ω–æ ‚Äî –Ω–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
        return None

    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...")
    result = train(
        csv_path="data/training_data.csv",
        out_dir=out_dir,
        categories_path=categories_path,
    )
    print("üéØ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:", result)
    return result


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Incremental retraining utility")
    parser.add_argument("--new-data", type=str, required=True, help="CSV —Å –Ω–æ–≤—ã–º–∏ confident –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏")
    parser.add_argument("--categories", type=str, default=None, help="Path –∫ categories.json (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")
    args = parser.parse_args()

    retrain_with_new_data(args.new_data, categories_path=args.categories)
